{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juanky/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_models import load_models, DNN, EnsembleDNN\n",
    "import matplotlib.pyplot as plt\n",
    "from attacks import pgd\n",
    "import random\n",
    "import cleverhans\n",
    "from cleverhans.utils_pytorch import convert_pytorch_model_to_tf\n",
    "from cleverhans.model import CallableModelWrapper\n",
    "from cleverhans.attacks import MomentumIterativeMethod\n",
    "import tensorflow as tf\n",
    "import torch.nn as nn\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_budget = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juanky/miniconda3/lib/python3.6/site-packages/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n"
     ]
    }
   ],
   "source": [
    "models = load_models('imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.load('experiment_data/imagenet/imagenet_images.pt').cuda()\n",
    "labels = torch.load('experiment_data/imagenet/imagenet_labels.pt').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[model.accuracy(images, labels, batch=True) for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = EnsembleDNN(models).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble.accuracy(images, labels, batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mim_attack(model, images, noise_budget, iters):\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    input_placeholder = tf.placeholder(tf.float32, shape=(None, 3, 224, 224,))\n",
    "    mim_params = {'eps': noise_budget,\n",
    "                  'eps_iter': noise_budget / iters,\n",
    "                  'nb_iter': iters,\n",
    "                  'clip_min': 0,\n",
    "                  'clip_max': 1, \n",
    "                  'ord': 2, \n",
    "                  'decay_factor': 1.0}\n",
    "    tf_model = convert_pytorch_model_to_tf(model, out_dims=1000)\n",
    "    cleverhans_model = CallableModelWrapper(tf_model, output_layer='logits')\n",
    "    mim = MomentumIterativeMethod(cleverhans_model, sess=sess)\n",
    "    adv_example_op = mim.generate(input_placeholder, **mim_params)\n",
    "    perturbed_images = []\n",
    "    for image in images:\n",
    "        adv_example = sess.run(adv_example_op, feed_dict={input_placeholder: image.unsqueeze(0)})\n",
    "        perturbed_images.append(adv_example[0])\n",
    "    return np.array(perturbed_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_accs_per_point(models, images, noise_vectors, labels):\n",
    "    model_accs_per_point = []\n",
    "    num_points = len(images)\n",
    "    for i in range(num_points):\n",
    "        x = images[i].unsqueeze(0)\n",
    "        v = noise_vectors[i].unsqueeze(0)\n",
    "        y = labels[i]\n",
    "        model_accs_per_point.append([model.accuracy(x + v, y) for model in models])\n",
    "    return np.array(model_accs_per_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_baseline = mim_attack(ensemble, images.cpu(), noise_budget, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_baseline = torch.tensor(ensemble_baseline).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_noise_vectors = ensemble_baseline - images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_accs_per_point = compute_model_accs_per_point(models, images, ensemble_noise_vectors, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.788555383682251"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([v.norm().item() for v in ensemble_noise_vectors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_accs_per_point.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Baseline\n",
      "Mean  0.262 Max  0.55\n"
     ]
    }
   ],
   "source": [
    "print(\"Ensemble Baseline\")\n",
    "print(\"Mean \", np.mean(np.mean(ensemble_accs_per_point, axis=1)), \n",
    "      \"Max \", np.mean(np.max(ensemble_accs_per_point, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Vector + Best Individual Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_baselines = [mim_attack(model, images.cpu(), noise_budget, 40) for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_baselines = torch.tensor(np.array(individual_baselines)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 100, 3, 224, 224])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "individual_baselines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_vector_baseline = torch.mean(individual_baselines, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 3, 224, 224])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_vector_baseline.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_vector_noise_vectors = average_vector_baseline - images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_vector_accs_per_point = compute_model_accs_per_point(models, images,\n",
    "                                                             average_vector_noise_vectors, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Vector Baseline\n",
      "Mean  0.5439999999999999 Max  0.84\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Vector Baseline\")\n",
    "print(\"Mean \", np.mean(np.mean(average_vector_accs_per_point, axis=1)), \n",
    "      \"Max \", np.mean(np.max(average_vector_accs_per_point, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_noise_vectors = torch.stack([individual_baselines[i] - images for i in range(len(models))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7824881672859192,\n",
       " 0.7879824042320251,\n",
       " 0.7826390862464905,\n",
       " 0.771580696105957,\n",
       " 0.7906122207641602]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[max([v.norm().item() for v in t]) for t in individual_noise_vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_best_individual_baseline(models, images, individual_noise_vectors, labels):\n",
    "    individual_accs_per_model = np.array([compute_model_accs_per_point(models, images, \n",
    "                                                                       individual_noise_vectors[i], labels)\n",
    "                                        for i in range(len(models))])\n",
    "    mean_max = []\n",
    "    for summary in [np.mean, np.max]:\n",
    "        overall = []\n",
    "        for i in range(len(images)):\n",
    "            point_accuracy_per_model = []\n",
    "            for m in range(len(models)):\n",
    "                r = summary(individual_accs_per_model[m][i])\n",
    "                point_accuracy_per_model.append(r)\n",
    "            overall.append(min(point_accuracy_per_model))\n",
    "        mean_max.append(np.mean(overall))\n",
    "    return mean_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Individual Baseline\n",
      "Mean Max [0.7019999999999998, 0.99]\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Individual Baseline\")\n",
    "print(\"Mean Max\", compute_best_individual_baseline(models, images, individual_noise_vectors, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = 'experiment_results/imagenet_1/'\n",
    "noise_vectors = torch.load(exp_dir + 'noise_vectors.pt')\n",
    "expected_losses = np.load(exp_dir + 'expected_losses.npy')\n",
    "minimum_losses = np.load(exp_dir + 'minimum_losses.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 100\n",
    "mwu_iters = 30\n",
    "model_accs_per_point = []\n",
    "for i in range(num_points):\n",
    "    x = images[i].unsqueeze(0)\n",
    "    y = labels[i]\n",
    "    model_accs = []\n",
    "    for t in range(mwu_iters):\n",
    "        v = noise_vectors[t][i].cuda()\n",
    "        model_accs.append([model.accuracy(x + v, y) for model in models])\n",
    "    model_accs_per_point.append(np.array(model_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_acc_plot = []\n",
    "mean_acc_plot = []\n",
    "for t in range(mwu_iters):\n",
    "    mean_acc = np.mean([np.mean(np.mean(model_accs[:t+1], axis=0)) for model_accs in model_accs_per_point])\n",
    "    max_acc = np.mean([np.max(np.mean(model_accs[:t+1], axis=0)) for model_accs in model_accs_per_point])\n",
    "    max_acc_plot.append(max_acc)\n",
    "    mean_acc_plot.append(mean_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_acc_plot[-1], mean_acc_plot[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(max_acc_plot, color='orange')\n",
    "plt.xlabel('MWU Iterations')\n",
    "plt.ylabel('Max Model Accuracy')\n",
    "plt.title('ImageNet')\n",
    "# plt.savefig('deep.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToPILImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=17\n",
    "ToPILImage(mode='RGB')(images[i].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ensemble_baseline[i] - images[i]).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ToPILImage(mode='RGB')(ensemble_baseline[i].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturb_im = ToPILImage(mode='RGB')(images[i].cpu() + noise_vectors[0][i][0].cpu())\n",
    "im.save('im_{}.png'.format(i))\n",
    "perturb_im.save('perturb_im_{}.png'.format(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
